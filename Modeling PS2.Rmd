---
title: "Modeling PS2"
author: "borui sun"
date: "1/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(rsample)
library(MASS)
library(e1071)
library(caret)
library(class)
library(pROC)
library("RColorBrewer")

theme_set(
  theme_bw()+
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(plot.subtitle = element_text(hjust = 0.5)) +
    theme(plot.tag.position = c(0.8, 0)) +
    theme(plot.tag = element_text(size=8)) +
    theme(strip.text.y = element_blank())
          )
```

### The Bayes Classifier

1. (20 points) For classification problems, the test error rate is minimized by a simple classifier that assigns each observation to the most likely class given its predictor values:

    $$\Pr(Y = j | X = x_0)$$
    
    where $x_0$ is the test observation and each possible class is represented by $J$. This is a **conditional probability** that $Y = j$, given the observed predictor vector $x_0$. This classifier is known as the **Bayes classifier**. If the response variable is binary (i.e. two classes), the Bayes classifier corresponds to predicting class one if $\Pr(Y = 1 | X = x_0) > 0.5$, and class two otherwise.
    
    Produce a graph illustrating this concept. Specifically, implement the following elements in your program:
        
    a. Set your random number generator seed.
    a. Simulate a dataset of $N = 200$ with $X_1, X_2$ where $X_1, X_2$ are random uniform variables between $[-1,1]$.
    a. Calculate $Y = X_1 + X_1^2 + X_2 + X_2^2 + \epsilon$, where $\epsilon \sim N(\mu = 0, \sigma^2 = 0.25)$.
    a. $Y$ is defined in terms of the log-odds of success on the domain $[-\infty, +\infty]$. Calculate the probability of success bounded between $[0,1]$.
    a. Plot each of the data points on a graph and use color to indicate if the observation was a success or a failure.
    a. Overlay the plot with Bayes decision boundary, calculated using $X_1, X_2$.
    a. Give your plot a meaningful title and axis labels.
    a. The colored background grid is optional.
 

```{r}
set.seed(123)

N = 200

simu_sample <- data.frame(
    x1 = runif(N, -1, 1),
    x2 = runif(N, -1, 1),
    epsilon = rnorm(N, mean = 0, sd = sqrt(0.25)) # sd = sigma
) %>%
    mutate(y = x1 + x1^2 + x2 + x2^2 + epsilon,
           pr = exp(y)/(1 + exp(y)), # as y <- log(pr/(1-pr))
           class = as.numeric(pr > 0.5))

bayes <- naiveBayes(as.factor(class) ~ x1 + x2, simu_sample)

sample_grid<- expand.grid(x1 = seq(-1, 1, length.out = 40), x2 = seq(-1, 1, length.out = 40)) %>%
    mutate(class = predict(bayes, sample_grid, type = "class"))

ggplot() + 
    geom_point(data = sample_grid, aes(x = x1, y = x2, color = as.factor(class)), alpha = 0.5, size = 0.5) +
    geom_point(data = simu_sample, aes(x = x1, y = x2, color =  as.factor(class))) +
    geom_contour(data = sample_grid, aes(x = x1, y = x2, z = as.numeric(class)),  bins = 2, binwidth = 0.01) +
    labs(title = "Bayes Decision Boundary", 
         x = "variable 1", 
         y = "variable 2",
         color = "class")  +
    scale_color_brewer(palette = "Set1")
```

2. (20 points) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?

    a. Repeat the following process 1000 times.
        i. Simulate a dataset of 1000 observations with $X_1, X_2 \sim \text{Uniform}(-1, +1)$. $Y$ is a binary response variable defined by a Bayes decision boundary of $f(X) = X_1 + X_2$, where values 0 or greater are coded `TRUE` and values less than 0 or coded `FALSE`. Whereas your simulated $Y$ is a function of $X_1 + X_2 + \epsilon$ where $\epsilon \sim N(0, 1)$. That is, your simulated $Y$ is a function of the Bayes decision boundary plus some irreducible error.
        ii. Randomly split your dataset into 70/30% training/test sets.
        iii. Use the training dataset to estimate LDA and QDA models.
        iv. Calculate each model's training and test error rate.
        
    b. Summarize all the simulations' error rates and report the results in tabular and graphical form. Use this evidence to support your answer.

```{r}
error_rate <- function(model, data, outcome){
    
    accuracy <- predict(model, data) %>%
        {.$class} %>%
        confusionMatrix(as.factor(data[[outcome]])) %>%
        {.$overall[1]}
    
    return(1 - accuracy)
}

repetition <- 1000

linear <- data.frame(matrix(nrow = 0, ncol = 0))

for (i in 1: repetition){
    
    N <- 1000 
    
    sample <- data.frame(x1 = runif(N, -1, 1),
                         x2 = runif(N, -1, 1),
                         epsilon = rnorm(N, mean = 0, sd =1)) %>%
        mutate(y = x1 + x2 + epsilon,
               class = y >= 0)

    split <- initial_split(sample, prop = .7)
    train <- training(split)
    test <- testing(split)
    
    lda <- lda(class ~ x1 + x2, data = train)
    
    lda_train_error <- error_rate(lda, train, "class")
    lda_test_error <- error_rate(lda, test, "class")
    
    qda <- qda(class ~ x1 + x2, data = train)
    
    qda_train_error <- error_rate(qda, train, "class")
    qda_test_error <- error_rate(qda, test, "class")
    
    id <- i
    result <- data.frame(id, lda_train_error, lda_test_error,
                                    qda_train_error, qda_test_error)
    linear <- bind_rows(linear, result)
}


linear %>% pivot_longer(
    cols = ends_with("error"),
    names_to = c("model", "data"),
    names_pattern = "(.*)_(.*)_error$",
    values_to = "error rate"
) %>% 
    ggplot(aes(x = `error rate`, fill = model)) +
    geom_density(alpha = 0.7) +
    facet_wrap(~data) +
    scale_fill_brewer(palette = "Set1")

```
    
```{r}
summary(linear)
```


3. (20 points) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?

    a. Repeat the following process 1000 times.
        i. Simulate a dataset of 1000 observations with $X_1, X_2 \sim \text{Uniform}(-1, +1)$. $Y$ is a binary response variable defined by a Bayes decision boundary of $f(X) = X_1 + X_1^2 + X_2 + X_2^2$, where values 0 or greater are coded `TRUE` and values less than 0 or coded `FALSE`. Whereas your simulated $Y$ is a function of $X_1 + X_1^2 + X_2 + X_2^2 + \epsilon$ where $\epsilon \sim N(0, 1)$. That is, your simulated $Y$ is a function of the Bayes decision boundary plus some irreducible error.
        ii. Randomly split your dataset into 70/30% training/test sets.
        iii. Use the training dataset to estimate LDA and QDA models.
        iv. Calculate each model's training and test error rate.
   
    b. Summarize all the simulations' error rates and report the results in tabular and graphical form. Use this evidence to support your answer.
    
```{r}
non_linear <- data.frame(matrix(nrow = 0, ncol = 0))

for (i in 1: repetition){
    
    N <- 1000 
    
    sample <- data.frame(x1 = runif(N, -1, 1),
                         x2 = runif(N, -1, 1),
                         epsilon = rnorm(N, mean = 0, sd =1)) %>%
        mutate(y = x1 + x1^2 + x2 + x2^2 + epsilon,
               class = y >= 0)
    
    split <- initial_split(sample, prop = .7)
    train <- training(split)
    test <- testing(split)
    
    lda <- lda(class ~ x1 + x1^2 + x2 + x2^2, data = train)
    
    lda_train_error <- error_rate(lda, train, "class")
    lda_test_error <- error_rate(lda, test, "class")
    
    qda <- qda(class ~ x1 + x1^2 + x2 + x2^2, data = train)
    
    qda_train_error <- error_rate(qda, train, "class")
    qda_test_error <- error_rate(qda, test, "class")
    
    id <- i
    result <- data.frame(id, lda_train_error, lda_test_error,
                                    qda_train_error, qda_test_error)
    
    non_linear <- bind_rows(non_linear, result)
}

summary(non_linear)

non_linear %>% pivot_longer(
    cols = ends_with("error"),
    names_to = c("model", "data"),
    names_pattern = "(.*)_(.*)_error$",
    values_to = "error rate"
) %>% 
    ggplot(aes(x = `error rate`, fill = model)) +
    geom_density(alpha = 0.7) +
    facet_wrap(~data) +
    scale_fill_brewer(palette = "Set1")
    
```


4. (20 points) In general, as sample size $n$ increases, do we expect the test error rate of QDA relative to LDA to improve, decline, or be unchanged? Why?
    a. Use the non-linear Bayes decision boundary approach from part (2) and vary $n$ across your simulations (e.g., simulate 1000 times for `n = c(1e02, 1e03, 1e04, 1e05`).
    b. Plot the test error rate for the LDA and QDA models as it changes over all of these values of $n$. _Use this graph to support your answer._
    
```{r}

sample_size <- c(100, 1000, 10000, 100000)

non_linear_diff_N <- data.frame(matrix(nrow = 0, ncol = 0))

for (i in seq_along(sample_size)){
    
    non_linear_simu <- data.frame(matrix(nrow = 0, ncol = 0))
    for (r in 1: repetition){
        
        N <- sample_size[i]
        sample <- data.frame(x1 = runif(N, -1, 1),
                         x2 = runif(N, -1, 1),
                         epsilon = rnorm(N, mean = 0, sd =1)) %>%
            mutate(y = x1 + x1^2 + x2 + x2^2 + epsilon,
                   class = y >= 0)
    
        split <- initial_split(sample, prop = .7)
        train <- training(split)
        test <- testing(split)
    
        lda <- lda(class ~ x1 + x1^2 + x2 + x2^2, data = train)
        lda_test_error <- error_rate(lda, test, "class")
    
        qda <- qda(class ~ x1 + x1^2 + x2 + x2^2, data = train)
        qda_test_error <- error_rate(qda, test, "class")
    
        id <- i
        result <- data.frame(id, lda_test_error, qda_test_error, N)
        non_linear_simu <- bind_rows(non_linear_simu, result)
    }
    non_linear_diff_N <- bind_rows(non_linear_diff_N, non_linear_simu)
}
non_linear_diff_N %>%
    pivot_longer(cols = ends_with("error"),
                 names_to = "model",
                 names_pattern = "(.*)_test_error",
                 values_to = "error") %>%
    ggplot(aes(x = error, fill = model)) +
    geom_density(alpha = 0.7) +
    facet_wrap(~N, scales = "free") +
    scale_fill_brewer(palette = "Set1")

```
    
### Modeling voter turnout

5. (20 points) Building several classifiers and comparing output.
    a. Split the data into a training and test set (70/30).
    b. Using the training set and all important predictors, estimate the following models with `vote96` as the response variable:
        i. Logistic regression model
        ii. Linear discriminant model
        iii. Quadratic discriminant model
        iv. Naive Bayes (you can use the default hyperparameter settings)
        v. $K$-nearest neighbors with $K = 1,2,\dots,10$ (that is, 10 separate models varying $K$) and _Euclidean_ distance metrics
    c. Using the test set, calculate the following model performance metrics:
        i. Error rate
        ii. ROC curve(s) / Area under the curve (AUC)
    d. Which model performs the best? Be sure to define what you mean by "best" and identify supporting evidence to support your conclusion(s).

```{r}
mental_health <- read_csv("mental_health.csv") %>%
    drop_na()
split <- initial_split(mental_health, prop = .7)
train <- training(split)
test <- testing(split)

# logistics regression
logit <- glm(vote96~., train, family = "binomial")
logit_predict <- as.numeric(predict(logit, test) > 0.5)

#lda
lda <- lda(vote96~., train)
lda_predict <- predict(lda, test)[["class"]] 

#qda 
qda <- qda(vote96~., train)
qda_predict <-predict(qda, test)[["class"]] 

#naive bayes
bayes <- naiveBayes(as.factor(vote96)~., train)
bayes_predict <- predict(bayes, test, type = "class") 


#knn 1- 10
knn_mse <- tibble(k = 1:10,
                  knn_predict = map(k, ~ knn(dplyr::select(train, -vote96),
                                          test = dplyr::select(test, -vote96),
                                          cl = train$vote96, k = .)))

roc_info <- roc(test$vote96, predict(logit, test), plot = TRUE,legacy.axes = TRUE)
roc_df <- data.frame(tp = roc_info$sensitivities,
                     fp = 1 - roc_info$specificities)

model_mse <- knn_mse %>%
    mutate(k = paste0("knn", k)) %>%
    `colnames<-`(c("model", "predict")) %>%
    bind_rows(tibble(model = "logit", predict = list(logit_predict))) %>%
    bind_rows(tibble(model = "lda", predict = list(lda_predict))) %>%
    bind_rows(tibble(model = "qda", predict = list(qda_predict))) %>%
    bind_rows(tibble(model = "bayes", predict = list(bayes_predict)))  %>%
    mutate(predict = map(predict, ~ as.numeric(as.character(.))),
           mse = map_dbl(predict, ~mean(test$vote96!=.)))

```

```{r, fig.height=10, fig.width=10}
roc_plot_prep <- data.frame(matrix(nrow = 0, ncol = 0))

for (i in 1:dim(model_mse)[1]){
    model <- model_mse$model[i]
    roc_info <- roc(test$vote96, model_mse$predict[[i]], legacy.axes = TRUE)
    roc_df <- tibble(model = model, 
                     tp = roc_info$sensitivities,
                     fp = 1 - roc_info$specificities,
                     auc = as.numeric(roc_info$auc))
    roc_plot_prep <- bind_rows(roc_plot_prep, roc_df)
    
    model_mse$auc[i] <- as.numeric(roc_info$auc)
}

roc_plot_prep %>% 
    mutate(auc = ifelse(tp ==0, auc, NA),
           model = factor(model, levels = c("logit", "lda", "qda", "bayes", paste0("knn", 1:10)))) %>%
    ggplot(aes(x = fp, y = tp, color = model)) +
    geom_line(size = 1.5) +
    facet_wrap(~model) +
    geom_line(data = rename(roc_plot_prep, model2 = model),
              aes(x = fp, y = tp, group  = model2), alpha = .3, size = 1, color = "grey50") +
    geom_label(aes(label = paste("AUC =", round(auc, 4))), 
            x = Inf, y = -Inf, hjust = 1, vjust = 0,
            inherit.aes = FALSE) +
    theme(legend.position = "none")
```